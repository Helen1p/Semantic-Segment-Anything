{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 265/265 [00:00<00:00, 572B/s]\n",
      "Downloading: 100%|██████████| 3.60k/3.60k [00:00<00:00, 8.15kB/s]\n",
      "Downloading: 100%|██████████| 58.0/58.0 [00:00<00:00, 134B/s]\n",
      "Downloading: 100%|██████████| 5.42k/5.42k [00:00<00:00, 12.8kB/s]\n",
      "Downloading: 100%|██████████| 3.71k/3.71k [00:00<00:00, 8.50kB/s]\n",
      "Downloading: 100%|██████████| 14.7k/14.7k [00:00<00:00, 36.7kB/s]\n",
      "Downloading: 100%|██████████| 117/117 [00:00<00:00, 270B/s]\n",
      "Downloading: 100%|██████████| 76.2k/76.2k [00:00<00:00, 148kB/s]\n",
      "Downloading: 100%|██████████| 123k/123k [00:00<00:00, 261kB/s]\n",
      "Downloading: 100%|██████████| 1.59M/1.59M [00:00<00:00, 2.41MB/s]\n",
      "Downloading: 100%|██████████| 1.75G/1.75G [02:39<00:00, 11.8MB/s]\n",
      "Downloading: 100%|██████████| 17.6k/17.6k [00:00<00:00, 44.8kB/s]\n",
      "Downloading: 100%|██████████| 15.2k/15.2k [00:00<00:00, 38.3kB/s]\n",
      "Downloading: 100%|██████████| 287/287 [00:00<00:00, 820B/s]\n",
      "Downloading: 100%|██████████| 42.8k/42.8k [00:00<00:00, 79.8kB/s]\n",
      "Downloading: 100%|██████████| 1.78M/1.78M [00:00<00:00, 2.21MB/s]\n",
      "Downloading: 100%|██████████| 498/498 [00:00<00:00, 1.12kB/s]\n",
      "Downloading: 100%|██████████| 2.95k/2.95k [00:00<00:00, 6.57kB/s]\n",
      "Downloading: 100%|██████████| 3.23M/3.23M [00:00<00:00, 4.71MB/s]\n"
     ]
    }
   ],
   "source": [
    "#验证SDK token\n",
    "from modelscope.hub.api import HubApi\n",
    "api = HubApi()\n",
    "api.login('f774872e-c878-4653-842e-d9d7e3982e47')\n",
    "\n",
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('OpenGVLab/InternVL2-1B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 179/179 [00:00<00:00, 390B/s]\n",
      "Downloading: 100%|██████████| 3.84k/3.84k [00:00<00:00, 6.06kB/s]\n",
      "Downloading: 100%|██████████| 58.0/58.0 [00:00<00:00, 88.2B/s]\n",
      "Downloading: 100%|██████████| 5.42k/5.42k [00:00<00:00, 11.0kB/s]\n",
      "Downloading: 100%|██████████| 6.84k/6.84k [00:00<00:00, 14.5kB/s]\n",
      "Downloading: 100%|██████████| 3.76k/3.76k [00:00<00:00, 10.9kB/s]\n",
      "Downloading: 100%|██████████| 14.7k/14.7k [00:00<00:00, 30.2kB/s]\n",
      "Downloading: 100%|██████████| 115/115 [00:00<00:00, 248B/s]\n",
      "Downloading: 100%|██████████| 76.2k/76.2k [00:00<00:00, 139kB/s]\n",
      "Downloading: 100%|██████████| 123k/123k [00:00<00:00, 181kB/s]\n",
      "Downloading: 100%|██████████| 4.11G/4.11G [05:24<00:00, 13.6MB/s]\n",
      "Downloading: 100%|██████████| 17.6k/17.6k [00:00<00:00, 35.2kB/s]\n",
      "Downloading: 100%|██████████| 59.8k/59.8k [00:00<00:00, 131kB/s]\n",
      "Downloading: 100%|██████████| 15.2k/15.2k [00:00<00:00, 38.6kB/s]\n",
      "Downloading: 100%|██████████| 287/287 [00:00<00:00, 708B/s]\n",
      "Downloading: 100%|██████████| 55.4k/55.4k [00:00<00:00, 106kB/s]\n",
      "Downloading: 100%|██████████| 1.78M/1.78M [00:00<00:00, 2.19MB/s]\n",
      "Downloading: 100%|██████████| 844/844 [00:00<00:00, 1.75kB/s]\n",
      "Downloading: 100%|██████████| 8.58k/8.58k [00:00<00:00, 23.1kB/s]\n",
      "Downloading: 100%|██████████| 7.61k/7.61k [00:00<00:00, 20.2kB/s]\n",
      "Downloading: 100%|██████████| 1.41M/1.41M [00:00<00:00, 2.17MB/s]\n",
      "Downloading: 100%|██████████| 3.91k/3.91k [00:00<00:00, 10.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "#验证SDK token\n",
    "from modelscope.hub.api import HubApi\n",
    "api = HubApi()\n",
    "api.login('f774872e-c878-4653-842e-d9d7e3982e47')\n",
    "\n",
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('OpenGVLab/InternVL2-2B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#验证SDK token\n",
    "from modelscope.hub.api import HubApi\n",
    "api = HubApi()\n",
    "api.login('f774872e-c878-4653-842e-d9d7e3982e47')\n",
    "\n",
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('OpenGVLab/InternVL2-8B', cache_dir='/root/autodl-tmp/pretrained/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 142/142 [00:00<00:00, 531B/s]\n",
      "Downloading: 100%|██████████| 3.63k/3.63k [00:00<00:00, 6.57kB/s]\n",
      "Downloading: 100%|██████████| 83.0/83.0 [00:00<00:00, 324B/s]\n",
      "Downloading: 100%|██████████| 5.42k/5.42k [00:00<00:00, 22.2kB/s]\n",
      "Downloading: 100%|██████████| 3.58k/3.58k [00:00<00:00, 13.4kB/s]\n",
      "Downloading: 100%|██████████| 14.7k/14.7k [00:00<00:00, 39.3kB/s]\n",
      "Downloading: 100%|██████████| 114/114 [00:00<00:00, 284B/s]\n",
      "Downloading: 100%|██████████| 76.2k/76.2k [00:00<00:00, 282kB/s]\n",
      "Downloading: 100%|██████████| 123k/123k [00:00<00:00, 319kB/s]\n",
      "Downloading: 100%|██████████| 4.65G/4.65G [01:10<00:00, 71.0MB/s]\n",
      "Downloading: 100%|██████████| 4.60G/4.60G [00:55<00:00, 89.7MB/s]\n",
      "Downloading: 100%|██████████| 4.49G/4.49G [00:55<00:00, 86.9MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:51<00:00, 92.9MB/s]\n",
      "Downloading: 100%|██████████| 4.65G/4.65G [00:54<00:00, 91.0MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:53<00:00, 89.7MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:53<00:00, 88.3MB/s]\n",
      "Downloading: 100%|██████████| 4.65G/4.65G [00:57<00:00, 86.9MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:50<00:00, 93.8MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:52<00:00, 91.1MB/s]\n",
      "Downloading: 100%|██████████| 4.65G/4.65G [00:53<00:00, 93.8MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:51<00:00, 92.9MB/s]\n",
      "Downloading: 100%|██████████| 4.43G/4.43G [00:50<00:00, 93.7MB/s]\n",
      "Downloading: 100%|██████████| 4.65G/4.65G [00:52<00:00, 95.3MB/s]\n",
      "Downloading:  36%|███▋      | 1.61G/4.43G [00:18<00:26, 114MB/s] 2024-09-09 13:38:08,831 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "Downloading:  39%|███▉      | 1.74G/4.43G [00:18<00:07, 386MB/s]2024-09-09 13:38:08,839 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:08,839 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:08,839 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:09,208 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:09,213 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:09,286 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:09,396 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "Downloading:  60%|█████▉    | 2.65G/4.43G [00:21<00:04, 431MB/s]2024-09-09 13:38:11,563 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:11,615 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:11,679 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:11,778 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "Downloading:  69%|██████▉   | 3.05G/4.43G [00:25<00:07, 203MB/s]2024-09-09 13:38:16,054 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:16,068 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:16,133 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:16,281 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "Downloading:  83%|████████▎ | 3.69G/4.43G [00:34<00:06, 125MB/s] 2024-09-09 13:38:24,441 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:24,582 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:24,604 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "2024-09-09 13:38:24,687 - modelscope - WARNING - Downloading: /root/autodl-tmp/pretrained/._____temp/OpenGVLab/InternVL2-40B/model-00015-of-00017.safetensors failed, reason: [Errno 28] No space left on device will retry\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "Downloading:  86%|████████▌ | 3.82G/4.43G [00:50<00:17, 38.6MB/s]"
     ]
    }
   ],
   "source": [
    "#验证SDK token\n",
    "from modelscope.hub.api import HubApi\n",
    "api = HubApi()\n",
    "api.login('f774872e-c878-4653-842e-d9d7e3982e47')\n",
    "\n",
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('OpenGVLab/InternVL2-40B', cache_dir='/root/autodl-tmp/pretrained/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 179/179 [00:00<00:00, 665B/s]\n",
      "Downloading: 100%|██████████| 3.84k/3.84k [00:00<00:00, 14.6kB/s]\n",
      "Downloading: 100%|██████████| 58.0/58.0 [00:00<00:00, 214B/s]\n",
      "Downloading: 100%|██████████| 5.42k/5.42k [00:00<00:00, 18.5kB/s]\n",
      "Downloading: 100%|██████████| 6.84k/6.84k [00:00<00:00, 19.6kB/s]\n",
      "Downloading: 100%|██████████| 3.76k/3.76k [00:00<00:00, 14.2kB/s]\n",
      "Downloading: 100%|██████████| 14.7k/14.7k [00:00<00:00, 29.1kB/s]\n",
      "Downloading: 100%|██████████| 115/115 [00:00<00:00, 430B/s]\n",
      "Downloading: 100%|██████████| 76.2k/76.2k [00:00<00:00, 274kB/s]\n",
      "Downloading: 100%|██████████| 123k/123k [00:00<00:00, 308kB/s]\n",
      "Downloading: 100%|██████████| 4.65G/4.65G [00:50<00:00, 99.3MB/s]\n",
      "Downloading: 100%|██████████| 4.60G/4.60G [00:47<00:00, 104MB/s] \n",
      "Downloading: 100%|██████████| 4.47G/4.47G [00:47<00:00, 100MB/s] \n",
      "Downloading: 100%|██████████| 4.55G/4.55G [00:48<00:00, 101MB/s] \n",
      "Downloading: 100%|██████████| 4.55G/4.55G [00:45<00:00, 108MB/s] \n",
      "Downloading: 100%|██████████| 4.64G/4.64G [00:49<00:00, 101MB/s] \n",
      "Downloading: 100%|██████████| 4.62G/4.62G [00:52<00:00, 94.4MB/s]\n",
      "Downloading: 100%|██████████| 4.55G/4.55G [00:48<00:00, 102MB/s] \n",
      "Downloading: 100%|██████████| 4.64G/4.64G [01:03<00:00, 78.2MB/s]\n",
      "Downloading: 100%|██████████| 4.62G/4.62G [01:22<00:00, 60.4MB/s]\n",
      "Downloading: 100%|██████████| 1.65G/1.65G [00:27<00:00, 64.9MB/s]\n",
      "Downloading: 100%|██████████| 82.0k/82.0k [00:00<00:00, 292kB/s]\n",
      "Downloading: 100%|██████████| 17.6k/17.6k [00:00<00:00, 49.1kB/s]\n",
      "Downloading: 100%|██████████| 59.8k/59.8k [00:00<00:00, 148kB/s]\n",
      "Downloading: 100%|██████████| 15.2k/15.2k [00:00<00:00, 45.1kB/s]\n",
      "Downloading: 100%|██████████| 287/287 [00:00<00:00, 787B/s]\n",
      "Downloading: 100%|██████████| 55.4k/55.4k [00:00<00:00, 146kB/s]\n",
      "Downloading: 100%|██████████| 1.78M/1.78M [00:00<00:00, 3.63MB/s]\n",
      "Downloading: 100%|██████████| 719/719 [00:00<00:00, 2.60kB/s]\n",
      "Downloading: 100%|██████████| 8.58k/8.58k [00:00<00:00, 34.8kB/s]\n",
      "Downloading: 100%|██████████| 7.61k/7.61k [00:00<00:00, 29.2kB/s]\n",
      "Downloading: 100%|██████████| 1.41M/1.41M [00:00<00:00, 4.00MB/s]\n",
      "Downloading: 100%|██████████| 3.79k/3.79k [00:00<00:00, 15.1kB/s]\n"
     ]
    }
   ],
   "source": [
    "#验证SDK token\n",
    "from modelscope.hub.api import HubApi\n",
    "api = HubApi()\n",
    "api.login('f774872e-c878-4653-842e-d9d7e3982e47')\n",
    "\n",
    "#模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('OpenGVLab/InternVL2-26B', cache_dir='/root/autodl-tmp/pretrained/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多卡时：\n",
    "# pip install transformers==4.37.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "# from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import math\n",
    "import json\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "\n",
    "def split_model(model_name):\n",
    "    device_map = {}\n",
    "    world_size = torch.cuda.device_count()\n",
    "    num_layers = {\n",
    "        'InternVL2-1B': 24, 'InternVL2-2B': 24, 'InternVL2-4B': 32, 'InternVL2-8B': 32,\n",
    "        'InternVL2-26B': 48, 'InternVL2-40B': 60, 'InternVL2-Llama3-76B': 80}[model_name]\n",
    "    # Since the first GPU will be used for ViT, treat it as half a GPU.\n",
    "    num_layers_per_gpu = math.ceil(num_layers / (world_size - 0.5))\n",
    "    num_layers_per_gpu = [num_layers_per_gpu] * world_size\n",
    "    num_layers_per_gpu[0] = math.ceil(num_layers_per_gpu[0] * 0.5)\n",
    "    layer_cnt = 0\n",
    "    for i, num_layer in enumerate(num_layers_per_gpu):\n",
    "        for j in range(num_layer):\n",
    "            device_map[f'language_model.model.layers.{layer_cnt}'] = i\n",
    "            layer_cnt += 1\n",
    "    device_map['vision_model'] = 0\n",
    "    device_map['mlp1'] = 0\n",
    "    device_map['language_model.model.tok_embeddings'] = 0\n",
    "    device_map['language_model.model.embed_tokens'] = 0\n",
    "    device_map['language_model.output'] = 0\n",
    "    device_map['language_model.model.norm'] = 0\n",
    "    device_map['language_model.lm_head'] = 0\n",
    "    device_map[f'language_model.model.layers.{num_layers - 1}'] = 0\n",
    "\n",
    "    return device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class chat_internvl(torch.nn.Module):\n",
    "    def __init__(self, model_path, img_prefix, semantic_prefix, json_prefix):\n",
    "        super().__init__()\n",
    "        self.img_prefix = img_prefix\n",
    "        self.semantic_prefix = semantic_prefix\n",
    "        self.json_prefix = json_prefix\n",
    "        split_model_path=model_path.split('/')[-1]\n",
    "        device_map = split_model(split_model_path)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map=device_map).eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_path = self.img_prefix+str(img)\n",
    "        semantic_path = self.semantic_prefix+img.split('.')[0]+'_semantic.png'\n",
    "        json_path = self.json_prefix+img.split('.')[0]+'_semantic.json'\n",
    "        with open(json_path) as f:\n",
    "            s=json.load(f)\n",
    "            number=len(s['annotations'])\n",
    "        generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "        pixel_values1 = load_image(semantic_path, max_num=12).to(torch.bfloat16).cuda()\n",
    "        pixel_values2 = load_image(img_path, max_num=12).to(torch.bfloat16).cuda()\n",
    "        pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "        num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "        \n",
    "        base = \"Image-1: <image>\\nImage-2: <image>\\nI want you to act as an image data annotator especially for quality assessment. \\\n",
    "I give you 2 images, they have same segment region divisions. Image-1 is just for reference, each segment region divisions of it has a colored mask, white boundaries and labeled with a number. The number starts from 0.\\\n",
    "Image-2 has some distortion in each region. For Image-2, you will be responsible to understand the content and region distortions referring to the region divisions of the image-1. \\\n",
    "Ignore the region that is not included in a region mask. All the demands below target for Image-2. I will give you some information about each region. \"\n",
    "        custom1 = \"There are {} regions. \".format(number)\n",
    "        custom2 = \"\"\n",
    "        for i in range(number):\n",
    "            name = s['annotations'][i]['class_name']\n",
    "            distortion = s['annotations'][i]['distortion']\n",
    "            distortions = ', '.join(distortion)\n",
    "            custom2 = custom2 + \"region {} is probably about {}, it has the distortion of {}. \".format(i,name, distortions)\n",
    "        # region object\n",
    "        example = \"1. Generate exactly 2 grounding question-answer pairs for Image-2. Start each question with \\\"What\\\" or \\\"How\\\", and provide the answer as a number corresponding to a region. There are some examples below but do not duplicate the examples. \\n \\\n",
    "Q: which is the most degraded region in all the region divisions? \\\n",
    "A: 3 \\\n",
    "Q: Which region has the most noticeable distortion of Gaussian noise? \\\n",
    "A: 2 \\\n",
    "Q: Which region is most affected by blur distortion? \\\n",
    "A: 2 \\\n",
    "Q: Which region is demonstrates both blur and jitter distortion? \\\n",
    "A: 1 \\\n",
    "Q: Which region shows the highest level of jitter distortion? \\\n",
    "A: 0 \\\n",
    "Q: What region shows the highest impact from JPEG compression?  \\\n",
    "A: 3\\\n",
    "Q: What region exhibits a little amount of jitter distortion? \\\n",
    "A: 1 \\\n",
    "Q: What region is slightly impacted by jitter distortion? \\\n",
    "A: 0 \\\n",
    "\"\n",
    "        question1 = base + custom1 + custom2 + example\n",
    "        response1, history = self.model.chat(self.tokenizer, pixel_values, question1, generation_config, num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "        print(f'Assistant: {response1}')\n",
    "        print('-----------------------')\n",
    "        question2 = '2. Generate exactly 2 referring question-answer pairs for Image-2. Begin each question with \\\"How\\\", \\\"What\\\" or \\\"Which\\\", and describe the content and location of the object in the question. Provide the answer as a single word describing the type of distortion, such as \\\"blur\\\". There are some examples below but do not duplicate the examples. \\n \\\n",
    "Q: what is the distortion of the boy to the left of the tree? \\\n",
    "A: blur \\\n",
    "Q: what is the distortion of the region in front of the house? \\\n",
    "A: overexposure \\\n",
    "Q: How is the distortion affecting the sky region above the house? \\\n",
    "A: blur \\\n",
    "Q: What type of distortion affects the boy standing near the left edge of the image? \\\n",
    "A: jpeg compression \\\n",
    "Q: Which distortion is present in the area around the house at the center of the image? \\\n",
    "A: jitter \\\n",
    "Q: How would you describe the distortion affecting the sky segment visible behind the boy? \\\n",
    "A: noise \\\n",
    "Q: What distortion is observed in the lower part of the image, specifically in the segment where the house is located? \\\n",
    "A: blur \\\n",
    "'\n",
    "        # question2 = '3. choose 1 regions to describe its content and location with more than 15 words.'\n",
    "        response2, history = self.model.chat(self.tokenizer, pixel_values, question2, generation_config, num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "        print(f'Assistant: {response2}')\n",
    "        print('-----------------------')\n",
    "        # instance\n",
    "        # prompt改一下\n",
    "        question3 = \"3. Give a long caption to describe this image by content and then by regions for Image-2, no more than 200 words.\"\n",
    "        response3, history = self.model.chat(self.tokenizer, pixel_values, question3, generation_config, num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "        print(f'Assistant: {response3}')\n",
    "        a={str(img): {'grounding': response1, 'referring': response2, 'caption': response3}}\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chat_internvl(torch.nn.Module):\n",
    "    def __init__(self, model_path, img_prefix, semantic_prefix, json_prefix):\n",
    "        super().__init__()\n",
    "        self.img_prefix = img_prefix\n",
    "        self.semantic_prefix = semantic_prefix\n",
    "        self.json_prefix = json_prefix\n",
    "        split_model_path=model_path.split('/')[-1]\n",
    "        device_map = split_model(split_model_path)\n",
    "        self.model = AutoModel.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_flash_attn=True,\n",
    "        trust_remote_code=True,\n",
    "        device_map=device_map).eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_path = self.img_prefix+str(img)\n",
    "        json_path = self.json_prefix+img.split('.')[0]+'_info.json'\n",
    "        with open(json_path) as f:\n",
    "            s=json.load(f)\n",
    "\n",
    "        \n",
    "        # prompt_gen=[]\n",
    "        # prompt_gen.append('There are {} main objects in this image'.format(len(s['annotations']))\n",
    "        # for i in s['annotations']:\n",
    "        #     dis_list=[]\n",
    "        #     for x, y in zip(i[distortion], i[distortion_level)]:\n",
    "        #         sstr = x+'(level '+ str(y)+')'\n",
    "        #         dis_list.append(sstr)\n",
    "        #     output_dis = (', ').join(dis_list)\n",
    "        #     'object {}: Bounding Box: [{},{}],[{},{}]; Semantic Label: {}; Distortion Type and level: {}' \\\n",
    "        # .format(i+1, i[bbox][0],i[bbox][1],i[bbox][2],i[bbox][3],i[class_name],output_dis)\n",
    "        \n",
    "        \n",
    "        pixel_values = load_image(img_path, max_num=12).to(torch.bfloat16).cuda()\n",
    "        generation_config = dict(max_new_tokens=1024, do_sample=False)\n",
    "        # 会给2个Description：semantic Description和distortion Description，global Description还行？\n",
    "        # 很奇怪，会影响第3个问题的回答，需要给第三个prompt加上The answer must be the distortion of the object. \\\n",
    "        question_base = '<image>\\n describe every object this image in detail, then give a whole description about this image and must include quality and distortion factors. '\n",
    "        \n",
    "        # 只给出整体semantic Description+distortion Description，global Description很垃圾\n",
    "        # question_base = '<image>\\n describe this image in detail and must include quality and distortion factors. '\n",
    "\n",
    "        prompt_gen = []\n",
    "        prompt_gen.append('There are {} main objects in this image'.format(len(s['annotations'])))\n",
    "        for idx, i in enumerate(s['annotations']):\n",
    "            dis_list = []\n",
    "            for x, y in zip(i['distortion'], i['distortion_level']):\n",
    "                sstr = x+'(level '+ str(y)+')'\n",
    "                dis_list.append(sstr)\n",
    "            output_dis = ('. ').join(dis_list)\n",
    "            sstr1 = 'object {}: Bounding Box: [{},{}],[{},{}]; Semantic Label: {}; Distortion Type and level: {}' \\\n",
    "        .format(str(idx+1), i['bbox'][0],i['bbox'][1],i['bbox'][2],i['bbox'][3],i['class_name'],output_dis)\n",
    "            prompt_gen.append(sstr1)\n",
    "        question_im = ('. ').join(prompt_gen)\n",
    "        \n",
    "        question = question_base + question_im\n",
    "        # question = question_base + question_im+'You should judge the semantic label is right or wrong. If wrong, evaluate to obtain the right semantic label by yourself.'\n",
    "\n",
    "        response, history = self.model.chat(self.tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "        print(f'User: {question}\\nAssistant: {response}')\n",
    "        \n",
    "        question = 'give a description about the spatial relations of {} main objects in this image.'.format(str(len(s['annotations'])))\n",
    "        response, history = self.model.chat(self.tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "        print(f'User: {question}\\nAssistant: {response}')\n",
    "        \n",
    "        # The answer must be the distortion of the object. \\            \n",
    "        question = 'use the spatial relations of {} main objects in the previous question to generate {} referring questions for {} objects. \\\n",
    "The question must include spatial relations of the object. \\\n",
    "The answer must be the distortion of the object. \\\n",
    "e.g. <question>: What is the distortion of the book in the lower-right corner? <answer>: jpeg compression(level 1)'.format(str(len(s['annotations'])),str(len(s['annotations'])),str(len(s['annotations'])))\n",
    "        response, history = self.model.chat(self.tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "        print(f'User: {question}\\nAssistant: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n",
      "Warning: Flash attention is not available, using eager attention instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1672cdaa4bb94f08b6caad67f1c6af00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ~/.cache/modelscope/hub\n",
    "chat=chat_internvl(model_path='/root/autodl-tmp/pretrained/OpenGVLab/InternVL2-26B', \n",
    "                   img_prefix='/root/autodl-tmp/example/DIV2K_output/', \n",
    "                   semantic_prefix='/root/autodl-tmp/example/semantic/', \n",
    "                   json_prefix='/root/autodl-tmp/example/json/', \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()  # 释放显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      " describe every object this image in detail, then give a whole description about this image and must include quality and distortion factors. There are 5 main objects in this image. object 1: Bounding Box: [0,37],[433,343]; Semantic Label: a keyboard; Distortion Type and level: lens blur(level 1). object 2: Bounding Box: [210,154],[297,228]; Semantic Label: a synthesizer; Distortion Type and level: white noise(level 2). object 3: Bounding Box: [82,232],[429,150]; Semantic Label: table; Distortion Type and level: over exposure(level 0). jitter(level 2). motion blur(level 2). object 4: Bounding Box: [417,17],[94,365]; Semantic Label: cabinet; Distortion Type and level: color block(level 1). lanczos interpolation resize(level 2). object 5: Bounding Box: [0,35],[252,114]; Semantic Label: table; Distortion Type and level: low light(level 2). contrast change(level 1)\n",
      "Assistant: ### Detailed Description of Objects in the Image\n",
      "\n",
      "#### Object 1: Bounding Box [0,37],[433,343]; Semantic Label: a keyboard; Distortion Type and level: lens blur(level 1)\n",
      "- **Description**: This object is a keyboard, which is a musical instrument used to play notes by pressing keys. The keys are arranged in a grid pattern, with white and black keys that correspond to different musical pitches. The keyboard has a sleek, modern design with a metallic finish.\n",
      "- **Distortion**: The keyboard is affected by a lens blur distortion, which causes a slight blurring effect, making the edges of the keys appear softened.\n",
      "\n",
      "#### Object 2: Bounding Box [210,154],[297,228]; Semantic Label: a synthesizer; Distortion Type and level: white noise(level 2)\n",
      "- **Description**: This object is a synthesizer, a device used to generate and manipulate sounds electronically. It has a series of knobs and buttons for adjusting various sound parameters. The synthesizer is compact and has a metallic finish.\n",
      "- **Distortion**: The synthesizer is affected by white noise distortion, which adds a grainy, textured appearance to the image, making the details of the knobs and buttons less clear.\n",
      "\n",
      "#### Object 3: Bounding Box [82,232],[429,150]; Semantic Label: table; Distortion Type and level: over exposure(level 0). jitter(level 2). motion blur(level 2).\n",
      "- **Description**: This object is a table, a flat surface used to support various items. The table appears to be made of wood and has a natural finish. It is positioned horizontally in the image.\n",
      "- **Distortion**: The table is affected by overexposure, jitter, and motion blur distortions. Overexposure causes the image to be too bright, making the details harder to discern. Jitter adds a slight wobbling effect, making the table appear unstable. Motion blur creates a blurred effect, making the edges of the table appear smudged.\n",
      "\n",
      "#### Object 4: Bounding Box [417,17],[94,365]; Semantic Label: cabinet; Distortion Type and level: color block(level 1). lanczos interpolation resize(level 2).\n",
      "- **Description**: This object is a cabinet, a piece of furniture used for storage. The cabinet is tall and has a wooden finish. It is positioned vertically in the image.\n",
      "- **Distortion**: The cabinet is affected by color block and Lanczos interpolation resize distortions. Color block causes the image to have blocks of solid color, making the details of the cabinet less clear. Lanczos interpolation resize adds a slight stretching effect, making the cabinet appear elongated.\n",
      "\n",
      "#### Object 5: Bounding Box [0,35],[252,114]; Semantic Label: table; Distortion Type and level: low light(level 2). contrast change(level 1)\n",
      "- **Description**: This object is another table, similar to the first one. It is also made of wood and has a natural finish. The table is positioned horizontally in the image.\n",
      "- **Distortion**: The table is affected by low light and contrast change distortions. Low light makes the image appear darker, making the details harder to discern. Contrast change alters the range of brightness in the image, making the table appear flatter in terms of visual depth.\n",
      "\n",
      "### Whole Description of the Image\n",
      "The image depicts a collection of musical equipment placed on a wooden table. The main objects in the image are a keyboard, a synthesizer, and two tables. The keyboard and synthesizer are positioned side by side, with the keyboard on the left and the synthesizer on the right. The tables are positioned behind the keyboard and synthesizer, with one table supporting the keyboard and the other supporting the synthesizer. The image has several distortions applied to different objects, including lens blur, white noise, overexposure, jitter, motion blur, color block, Lanczos interpolation resize, low light, and contrast change. These distortions affect the clarity and visual quality of the objects, making some details harder to discern. The overall quality of the image is somewhat compromised due to these distortions.\n",
      "User: give a description about the spatial relation of 5 main objects in this image.\n",
      "Assistant: ### Spatial Relations of the 5 Main Objects in the Image\n",
      "\n",
      "1. **Object 1 (Keyboard)**:\n",
      "   - **Position**: The keyboard is positioned on the left side of the image.\n",
      "   - **Orientation**: It is placed horizontally, with the keys facing towards the right side of the image.\n",
      "   - **Relation to Other Objects**: The keyboard is adjacent to the synthesizer, with a small gap between them. It is also supported by the table behind it.\n",
      "\n",
      "2. **Object 2 (Synthesizer)**:\n",
      "   - **Position**: The synthesizer is positioned on the right side of the image.\n",
      "   - **Orientation**: It is placed horizontally, with the knobs and buttons facing towards the left side of the image.\n",
      "   - **Relation to Other Objects**: The synthesizer is adjacent to the keyboard, with a small gap between them. It is also supported by the table behind it.\n",
      "\n",
      "3. **Object 3 (Table 1)**:\n",
      "   - **Position**: The table is positioned behind the keyboard and synthesizer.\n",
      "   - **Orientation**: It is placed horizontally, with the surface facing upwards.\n",
      "   - **Relation to Other Objects**: The table supports the keyboard and synthesizer, providing a stable base for both objects.\n",
      "\n",
      "4. **Object 4 (Cabinet)**:\n",
      "   - **Position**: The cabinet is positioned to the right of the synthesizer.\n",
      "   - **Orientation**: It is placed vertically, with the front facing towards the right side of the image.\n",
      "   - **Relation to Other Objects**: The cabinet is adjacent to the synthesizer, with a small gap between them. It is also supported by the table behind it.\n",
      "\n",
      "5. **Object 5 (Table 2)**:\n",
      "   - **Position**: The table is positioned behind the synthesizer and cabinet.\n",
      "   - **Orientation**: It is placed horizontally, with the surface facing upwards.\n",
      "   - **Relation to Other Objects**: The table supports the synthesizer and cabinet, providing a stable base for both objects. It is also adjacent to the first table, which supports the keyboard.\n",
      "\n",
      "### Summary\n",
      "- The keyboard and synthesizer are the primary musical instruments in the image, positioned side by side on a table.\n",
      "- The two tables provide support for the keyboard, synthesizer, and cabinet, ensuring stability.\n",
      "- The cabinet is positioned to the right of the synthesizer, also supported by the table.\n",
      "- The spatial arrangement of the objects is orderly, with each item having a clear position relative to the others, creating a cohesive setup.\n",
      "User: use the spatial relation of 5 main objects to generate 5 referring questions for 5 objects. The answer must be the distortion of the object. e.g. <question>: What is the distortion of the book in the lower-right corner? <answer>: jpeg compression(level 1)\n",
      "Assistant: ### Referring Questions for the 5 Main Objects\n",
      "\n",
      "1. **Question**: What is the distortion of the keyboard in the left side of the image?\n",
      "   **Answer**: Lens blur (level 1)\n",
      "\n",
      "2. **Question**: What is the distortion of the synthesizer on the right side of the image?\n",
      "   **Answer**: White noise (level 2)\n",
      "\n",
      "3. **Question**: What is the distortion of the table behind the keyboard?\n",
      "   **Answer**: Overexposure (level 0), jitter (level 2), motion blur (level 2)\n",
      "\n",
      "4. **Question**: What is the distortion of the cabinet to the right of the synthesizer?\n",
      "   **Answer**: Color block (level 1), Lanczos interpolation resize (level 2)\n",
      "\n",
      "5. **Question**: What is the distortion of the table behind the synthesizer and cabinet?\n",
      "   **Answer**: Low light (level 2), contrast change (level 1)\n"
     ]
    }
   ],
   "source": [
    "chat('121-cc-774921.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      " describe every object this image in detail, then give a whole description about this image and must include quality and distortion factors. There are 4 main objects in this image. object 1: Bounding Box: [0,0],[511,381]; Semantic Label: a building; Distortion Type and level: gaussian blur(level 1). mean shift(level 2). object 2: Bounding Box: [295,238],[71,144]; Semantic Label: a fire hydrant; Distortion Type and level: color diffuse(level 0). object 3: Bounding Box: [402,250],[92,83]; Semantic Label: a wooden fence; Distortion Type and level: motion blur(level 2). color saturate(level 1). gaussian blur(level 1). object 4: Bounding Box: [365,332],[146,50]; Semantic Label: plant; Distortion Type and level: low light(level 2). color saturate(level 2). bilinear interpolation resize(level 2)\n",
      "Assistant: ### Detailed Description of Objects in the Image\n",
      "\n",
      "#### Object 1: Building\n",
      "- **Bounding Box:** [0,0],[511,381]\n",
      "- **Semantic Label:** Building\n",
      "- **Distortion Type and Level:** Gaussian Blur (level 1), Mean Shift (level 2)\n",
      "\n",
      "The building in the image appears to be an old, possibly abandoned structure. The walls are white but show signs of wear and tear, with peeling paint and visible cracks. The windows are large and rectangular, with multiple panes, and they are covered in dirt and grime, indicating neglect. The glass is slightly fogged, adding to the overall aged and worn appearance of the building. The roof and other parts of the building are not visible in this image.\n",
      "\n",
      "#### Object 2: Fire Hydrant\n",
      "- **Bounding Box:** [295,238],[71,144]\n",
      "- **Semantic Label:** Fire Hydrant\n",
      "- **Distortion Type and Level:** Color Diffuse (level 0)\n",
      "\n",
      "The fire hydrant is located in the foreground of the image. It is a standard, cylindrical red fire hydrant with a black top. The hydrant appears to be in good condition, with no visible signs of damage or wear. It is positioned on a patch of grass, which is slightly overgrown, indicating that the area might not be frequently maintained.\n",
      "\n",
      "#### Object 3: Wooden Fence\n",
      "- **Bounding Box:** [402,250],[92,83]\n",
      "- **Semantic Label:** Wooden Fence\n",
      "- **Distortion Type and Level:** Motion Blur (level 2), Color Saturate (level 1), Gaussian Blur (level 1)\n",
      "\n",
      "The wooden fence is partially visible in the image, located to the right of the fire hydrant. It is made of vertical wooden planks and appears to be in a state of disrepair. The wood is weathered and shows signs of rot and decay. The fence is slightly blurred, indicating motion blur, and the colors are somewhat desaturated, giving it a faded and worn-out look.\n",
      "\n",
      "#### Object 4: Plant\n",
      "- **Bounding Box:** [365,332],[146,50]\n",
      "- **Semantic Label:** Plant\n",
      "- **Distortion Type and Level:** Low Light (level 2), Color Saturate (level 2), Bilinear Interpolation Resize (level 2)\n",
      "\n",
      "The plant is a tall, slender plant with a long, thin stem and a cluster of small, yellow flowers at the top. It is located in front of the building, slightly to the left of the fire hydrant. The plant appears to be growing in a small patch of soil, and it is somewhat blurred, indicating low light conditions. The colors of the plant are somewhat saturated, making the yellow flowers stand out against the muted background.\n",
      "\n",
      "### Whole Description of the Image\n",
      "\n",
      "The image depicts an old, possibly abandoned building with a sign indicating it is a \"Motor Vehicle Inspection Station.\" The building shows significant signs of wear and tear, with peeling paint, cracks, and dirty windows. In front of the building, there is a red fire hydrant in good condition, positioned on a slightly overgrown patch of grass. To the right of the fire hydrant, a weathered wooden fence is partially visible, showing signs of decay. In front of the building, a tall, slender plant with yellow flowers is growing, slightly blurred due to low light conditions. The overall quality of the image is somewhat low, with noticeable distortions such as blurring, color saturation changes, and low light effects. The image has a somewhat eerie and neglected atmosphere, emphasizing the abandonment and decay of the building and its surroundings.\n",
      "User: give a description about the spatial relations of 4 main objects in this image.\n",
      "Assistant: ### Spatial Relations of the 4 Main Objects in the Image\n",
      "\n",
      "#### Object 1: Building\n",
      "- **Position:** The building occupies the left side of the image, extending from the top to the bottom.\n",
      "- **Relation to Other Objects:** The building is the primary structure in the image and serves as the backdrop for the other objects. It is directly behind the fire hydrant and the plant, with the wooden fence partially visible to the right of the building.\n",
      "\n",
      "#### Object 2: Fire Hydrant\n",
      "- **Position:** The fire hydrant is located in the foreground, slightly to the right of the center of the image.\n",
      "- **Relation to Other Objects:** The fire hydrant is positioned in front of the building, with the plant growing in front of it. The wooden fence is partially visible to the right of the fire hydrant.\n",
      "\n",
      "#### Object 3: Wooden Fence\n",
      "- **Position:** The wooden fence is located to the right of the fire hydrant, extending vertically in the image.\n",
      "- **Relation to Other Objects:** The fence is partially visible and appears to be in a state of disrepair. It is positioned behind the fire hydrant and in front of the building.\n",
      "\n",
      "#### Object 4: Plant\n",
      "- **Position:** The plant is located in front of the building, slightly to the left of the fire hydrant.\n",
      "- **Relation to Other Objects:** The plant is growing in a small patch of soil in front of the building, with the fire hydrant positioned directly behind it. The wooden fence is partially visible to the right of the plant.\n",
      "\n",
      "### Summary of Spatial Relations\n",
      "- The building is the primary structure in the image, providing a backdrop for the other objects.\n",
      "- The fire hydrant is in the foreground, positioned in front of the building and slightly to the right of the center.\n",
      "- The wooden fence is partially visible to the right of the fire hydrant, in a state of disrepair.\n",
      "- The plant is growing in front of the building, slightly to the left of the fire hydrant.\n",
      "\n",
      "These spatial relations create a layered composition, with the building as the dominant structure, the fire hydrant and plant in the foreground, and the wooden fence partially visible to the right. The arrangement gives a sense of depth and context to the scene, emphasizing the neglected and abandoned nature of the area.\n",
      "User: use the spatial relations of 4 main objects in the previous question to generate 4 referring questions for 4 objects. The question must include spatial relations of the object. The answer must be the distortion of the object. e.g. <question>: What is the distortion of the book in the lower-right corner? <answer>: jpeg compression(level 1)\n",
      "Assistant: ### Referring Questions for the 4 Main Objects\n",
      "\n",
      "#### Object 1: Building\n",
      "**Question:** What is the distortion of the building in the left side of the image?\n",
      "**Answer:** Gaussian Blur (level 1), Mean Shift (level 2)\n",
      "\n",
      "#### Object 2: Fire Hydrant\n",
      "**Question:** What is the distortion of the fire hydrant in the foreground, slightly to the right of the center?\n",
      "**Answer:** Color Diffuse (level 0)\n",
      "\n",
      "#### Object 3: Wooden Fence\n",
      "**Question:** What is the distortion of the wooden fence to the right of the fire hydrant?\n",
      "**Answer:** Motion Blur (level 2), Color Saturate (level 1), Gaussian Blur (level 1)\n",
      "\n",
      "#### Object 4: Plant\n",
      "**Question:** What is the distortion of the plant in front of the building, slightly to the left of the fire hydrant?\n",
      "**Answer:** Low Light (level 2), Color Saturate (level 2), Bilinear Interpolation Resize (level 2)\n",
      "\n",
      "### Summary of Referring Questions and Distortions\n",
      "1. **Building:** Gaussian Blur (level 1), Mean Shift (level 2)\n",
      "2. **Fire Hydrant:** Color Diffuse (level 0)\n",
      "3. **Wooden Fence:** Motion Blur (level 2), Color Saturate (level 1), Gaussian Blur (level 1)\n",
      "4. **Plant:** Low Light (level 2), Color Saturate (level 2), Bilinear Interpolation Resize (level 2)\n",
      "\n",
      "These questions and answers provide a clear understanding of the spatial relations and distortions of each object in the image.\n"
     ]
    }
   ],
   "source": [
    "chat('abandoned-1596853.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use flash attn: False\n"
     ]
    }
   ],
   "source": [
    "major, minor = torch.cuda.get_device_capability(0)\n",
    "# Check if the GPU architecture is Ampere (SM 8.x) or newer (SM 9.0)\n",
    "is_sm8x = major == 8 and minor >= 0\n",
    "is_sm90 = major == 9 and minor == 0\n",
    "print('Use flash attn:', is_sm90 or is_sm90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-08-30T09:42:41.864350Z",
     "iopub.status.busy": "2024-08-30T09:42:41.864025Z",
     "iopub.status.idle": "2024-08-30T09:42:49.864421Z",
     "shell.execute_reply": "2024-08-30T09:42:49.863776Z",
     "shell.execute_reply.started": "2024-08-30T09:42:41.864330Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Q1: Which region has the most noticeable distortion of Gaussian noise?\n",
      "A1: 3\n",
      "\n",
      "Q2: Which region has the most noticeable distortion of JPEG compression?\n",
      "A2: 4\n",
      "-----------------------\n",
      "Assistant: Q1: What is the distortion of the region in front of the house?\n",
      "A1: overexposure\n",
      "\n",
      "Q2: What is the distortion of the region in front of the house?\n",
      "A2: blur\n",
      "-----------------------\n",
      "Assistant: This image depicts a cluttered outdoor scene with various objects and items scattered across a cobblestone surface. The scene is characterized by a mix of vibrant colors and textures, creating a visually stimulating environment. The ground is covered with a pattern of red and blue tiles, adding a dynamic contrast to the scene. In the foreground, there is a large, green, woven basket, which appears to be slightly disorganized, with some items protruding from it. Nearby, a black plastic chair is partially visible, suggesting a casual, perhaps recreational setting. The background features a variety of objects, including a large, black trash can, a purple plastic chair, and a green plastic container, all contributing to the cluttered appearance. The scene is further enhanced by the presence of a large, black, cylindrical object, possibly a trash bin or a container, which adds to the sense of disarray. The overall impression is one of a busy, possibly neglected outdoor area, with a mix of functional and decorative elements contributing to a lively yet chaotic atmosphere.\n",
      "Assistant: ### Grounding Questions and Answers for Image-2\n",
      "\n",
      "**Question 1: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 1: 2**\n",
      "\n",
      "**Question 2: Which region has the most noticeable distortion of JPEG compression jitter?**\n",
      "\n",
      "**Answer 2: 3**\n",
      "\n",
      "**Question 3: Which region has the most noticeable distortion of over-exposure?**\n",
      "\n",
      "**Answer 3: 0**\n",
      "\n",
      "**Question 4: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 4: 2**\n",
      "\n",
      "**Question 5: Which region has the most noticeable distortion of green leaves?**\n",
      "\n",
      "**Answer 5: 1**\n",
      "\n",
      "**Question 6: Which region has the most noticeable distortion of a green plant?**\n",
      "\n",
      "**Answer 6: 3**\n",
      "\n",
      "**Question 7: Which region has the most noticeable distortion of a leafy plant?**\n",
      "\n",
      "**Answer 7: 0**\n",
      "\n",
      "**Question 8: Which region has the most noticeable distortion of a leaf with a distorted shape?**\n",
      "\n",
      "**Answer 8: 1**\n",
      "\n",
      "**Question 9: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 9: 2**\n",
      "\n",
      "**Question 10: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 10: 3**\n",
      "\n",
      "**Question 11: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 11: 4**\n",
      "\n",
      "**Question 12: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 12: 5**\n",
      "\n",
      "**Question 13: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 13: 6**\n",
      "\n",
      "**Question 14: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 14: 7**\n",
      "\n",
      "**Question 15: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape?**\n",
      "\n",
      "**Answer 15: 8**\n",
      "\n",
      "**Question 16: Which region has the most noticeable distortion of a leaf with a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a distorted shape and a\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (9205 > 8192). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ### Grounding Questions and Answers for Image-2\n",
      "\n",
      "**Question 1: What is the distortion of the region labeled 2?**\n",
      "\n",
      "**Answer 1: Overexposure**\n",
      "\n",
      "**Question 2: What is the distortion of the region labeled 3?**\n",
      "\n",
      "**Answer 2: Noise**\n",
      "\n",
      "**Question 3: What is the distortion of the region labeled 1?**\n",
      "\n",
      "**Answer 3: Noise**\n",
      "\n",
      "**Question 4: What is the distortion of the region labeled 0?**\n",
      "\n",
      "**Answer 4: Overexposure**\n",
      "\n",
      "**Question 5: What is the distortion of the region labeled 4?**\n",
      "\n",
      "**Answer 5: Noise**\n",
      "\n",
      "**Question 6: What is the distortion of the region labeled 5?**\n",
      "\n",
      "**Answer 6: Overexposure**\n",
      "\n",
      "**Question 7: What is the distortion of the region labeled 6?**\n",
      "\n",
      "**Answer 7: Overexposure**\n",
      "\n",
      "**Question 8: What is the distortion of the region labeled 7?**\n",
      "\n",
      "**Answer 8: Overexposure**\n",
      "\n",
      "**Question 9: What is the distortion of the region labeled 8?**\n",
      "\n",
      "**Answer 9: Overexposure**\n",
      "\n",
      "**Question 10: What is the distortion of the region labeled 9?**\n",
      "\n",
      "**Answer 10: Overexposure**\n",
      "\n",
      "**Question 11: What is the distortion of the region labeled 10?**\n",
      "\n",
      "**Answer 11: Overexposure**\n",
      "\n",
      "**Question 12: What is the distortion of the region labeled 11?**\n",
      "\n",
      "**Answer 12: Overexposure**\n",
      "\n",
      "**Question 13: What is the distortion of the region labeled 12?**\n",
      "\n",
      "**Answer 13: Overexposure**\n",
      "\n",
      "**Question 14: What is the distortion of the region labeled 13?**\n",
      "\n",
      "**Answer 14: Overexposure**\n",
      "\n",
      "**Question 15: What is the distortion of the region labeled 14?**\n",
      "\n",
      "**Answer 15: Overexposure**\n",
      "\n",
      "**Question 16: What is the distortion of the region labeled 15?**\n",
      "\n",
      "**Answer 16: Overexposure**\n",
      "\n",
      "**Question 17: What is the distortion of the region labeled 16?**\n",
      "\n",
      "**Answer 17: Overexposure**\n",
      "\n",
      "**Question 18: What is the distortion of the region labeled 17?**\n",
      "\n",
      "**Answer 18: Overexposure**\n",
      "\n",
      "**Question 19: What is the distortion of the region labeled 18?**\n",
      "\n",
      "**Answer 19: Overexposure**\n",
      "\n",
      "**Question 20: What is the distortion of the region labeled 19?**\n",
      "\n",
      "**Answer 20: Overexposure**\n",
      "\n",
      "**Question 21: What is the distortion of the region labeled 20?**\n",
      "\n",
      "**Answer 21: Overexposure**\n",
      "\n",
      "**Question 22: What is the distortion of the region labeled 21?**\n",
      "\n",
      "**Answer 22: Overexposure**\n",
      "\n",
      "**Question 23: What is the distortion of the region labeled 22?**\n",
      "\n",
      "**Answer 23: Overexposure**\n",
      "\n",
      "**Question 24: What is the distortion of the region labeled 23?**\n",
      "\n",
      "**Answer 24: Overexposure**\n",
      "\n",
      "**Question 25: What is the distortion of the region labeled 24?**\n",
      "\n",
      "**Answer 25: Overexposure**\n",
      "\n",
      "**Question 26: What is the distortion of the region labeled 25?**\n",
      "\n",
      "**Answer 26: Overexposure**\n",
      "\n",
      "**Question 27: What is the distortion of the region labeled 26?**\n",
      "\n",
      "**Answer 27: Overexposure**\n",
      "\n",
      "**Question 28: What is the distortion of the region labeled 27?**\n",
      "\n",
      "**Answer 28: Overexposure**\n",
      "\n",
      "**Question 29: What is the distortion of the region labeled 28?**\n",
      "\n",
      "**Answer 29: Overexposure**\n",
      "\n",
      "**Question 30: What is the distortion of the region labeled 29?**\n",
      "\n",
      "**Answer 30: Overexposure**\n",
      "\n",
      "**Question 31: What is the distortion of the region labeled 30?**\n",
      "\n",
      "**Answer 31: Overexposure**\n",
      "\n",
      "**Question 32: What is the distortion of the region labeled 31?**\n",
      "\n",
      "**Answer 32: Overexposure**\n",
      "\n",
      "**Question 33: What is the distortion of the region labeled 32?**\n",
      "\n",
      "**Answer 33: Overexposure**\n",
      "\n",
      "**Question 34: What is the distortion of the region labeled 33?**\n",
      "\n",
      "**Answer 34: Overexposure**\n",
      "\n",
      "**Question 35: What is the distortion of the region labeled 34?**\n",
      "\n",
      "**Answer 35: Overexposure**\n",
      "\n",
      "**Question 36: What is the distortion of the region labeled 35?**\n",
      "\n",
      "**Answer 36: Overexposure**\n",
      "\n",
      "**Question 37: What is the distortion of the region labeled 36?**\n",
      "\n",
      "**Answer 37: Overexposure**\n",
      "\n",
      "**Question 38: What is the distortion of the region labeled 37?**\n",
      "\n",
      "**Answer 38: Overexposure**\n",
      "\n",
      "**Question 39: What is the distortion of the region labeled 38?**\n",
      "\n",
      "**Answer 39: Overexposure**\n",
      "\n",
      "**Question 40:\n",
      "-----------------------\n",
      "Assistant: **Image-2: A close-up view of a lush, green plant with distinct, large, and slightly distorted leaves. The leaves exhibit a vibrant green hue, typical of healthy foliage. The central part of the image, where the leaves meet, is slightly more illuminated, creating a contrast that highlights the texture and veins of the leaves. The background is filled with other similar green plants, creating a dense, verdant environment. The overall scene is one of natural beauty and abundance, with the focus on the intricate details of the leaves and the lush greenery.**\n",
      "\n",
      "**Caption: A Detailed Examination of the Distorted Leaves in Image-2**\n",
      "\n",
      "**Content Description:**\n",
      "The image showcases a close-up view of a plant with large, slightly distorted leaves. The leaves are a vibrant green, indicating a healthy plant. The central part of the image, where the leaves meet, is slightly more illuminated, creating a contrast that highlights the texture and veins of the leaves. The background is filled with other similar green plants, creating a dense, verdant environment. The overall scene is one of natural beauty and abundance, with the focus on the intricate details of the leaves and the lush greenery.\n",
      "Assistant: Q1: Which region has the most noticeable distortion of blur?\n",
      "A1: 4\n",
      "\n",
      "Q2: Which region has the most noticeable distortion of low light?\n",
      "A2: 5\n",
      "-----------------------\n",
      "Assistant: Q1: What is the distortion of the region in front of the house?\n",
      "A1: overexposure\n",
      "\n",
      "Q2: What is the distortion of the region in front of the house?\n",
      "A2: overexposure\n",
      "-----------------------\n",
      "Assistant: This image showcases a collection of colorful markers, each with distinct colors and labeled with the brand name \"Hyperbright.\" The markers are arranged in a row, each with a unique color scheme and a slight distortion in their appearance. The background is a plain, light color, providing a stark contrast that highlights the markers' vibrant hues. The markers exhibit a range of distortions, from slight blurring to noticeable pixelation, which are evident in the regions marked with numbers 0, 1, 2, 3, 4, 5, and 6. These distortions are caused by various factors, including overexposure, low light, and noise, which affect the clarity and sharpness of the markers' images. The overall composition of the image emphasizes the variety of colors and the unique characteristics of each marker, while also highlighting the common issues that arise due to these distortions.\n",
      "Assistant: ### Grounding Questions and Answers for Image-2\n",
      "\n",
      "**Question 1: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 1: 2**\n",
      "\n",
      "**Question 2: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 2: 0**\n",
      "\n",
      "**Question 3: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 3: 1**\n",
      "\n",
      "**Question 4: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 4: 1**\n",
      "\n",
      "**Question 5: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 5: 0**\n",
      "\n",
      "**Question 6: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 6: 1**\n",
      "\n",
      "**Question 7: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 7: 2**\n",
      "\n",
      "**Question 8: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 8: 3**\n",
      "\n",
      "**Question 9: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 9: 2**\n",
      "\n",
      "**Question 10: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 10: 1**\n",
      "\n",
      "**Question 11: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 11: 0**\n",
      "\n",
      "**Question 12: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 12: 1**\n",
      "\n",
      "**Question 13: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 13: 2**\n",
      "\n",
      "**Question 14: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 14: 3**\n",
      "\n",
      "**Question 15: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 15: 0**\n",
      "\n",
      "**Question 16: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 16: 1**\n",
      "\n",
      "**Question 17: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 17: 2**\n",
      "\n",
      "**Question 18: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 18: 1**\n",
      "\n",
      "**Question 19: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 19: 2**\n",
      "\n",
      "**Question 20: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 20: 3**\n",
      "\n",
      "**Question 21: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 21: 0**\n",
      "\n",
      "**Question 22: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 22: 1**\n",
      "\n",
      "**Question 23: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 23: 2**\n",
      "\n",
      "**Question 24: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 24: 1**\n",
      "\n",
      "**Question 25: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 25: 2**\n",
      "\n",
      "**Question 26: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 26: 3**\n",
      "\n",
      "**Question 27: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 27: 0**\n",
      "\n",
      "**Question 28: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 28: 1**\n",
      "\n",
      "**Question 29: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 29: 2**\n",
      "\n",
      "**Question 30: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 30: 1**\n",
      "\n",
      "**Question 31: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 31: 2**\n",
      "\n",
      "**Question 32: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 32: 3**\n",
      "\n",
      "**Question 33: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 33: 0**\n",
      "\n",
      "**Question 34: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 34: 1**\n",
      "\n",
      "**Question 35: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 35: 2**\n",
      "\n",
      "**Question 36: Which region has the most noticeable distortion of noise?**\n",
      "\n",
      "**Answer 36: 1**\n",
      "\n",
      "**Question 37: Which region has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 37: 2**\n",
      "\n",
      "**Question 38: Which region has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 38: 3**\n",
      "\n",
      "**Question 39: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 39: 0**\n",
      "\n",
      "**Question 40: Which region has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 40: 1**\n",
      "\n",
      "**Question 41: Which region has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 41: 2**\n",
      "\n",
      "**Question 42: Which region has\n",
      "-----------------------\n",
      "Assistant: **Question 1: What is the distortion of the sky in Image-2?**\n",
      "\n",
      "**Answer 1: blur**\n",
      "\n",
      "**Question 2: Which region in Image-2 has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 2: 1**\n",
      "\n",
      "**Question 3: What is the distortion of the grass in Image-2?**\n",
      "\n",
      "**Answer 3: noise**\n",
      "\n",
      "**Question 4: Which region in Image-2 has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 4: 1**\n",
      "\n",
      "**Question 5: What is the distortion of the mountain in Image-2?**\n",
      "\n",
      "**Answer 5: low light**\n",
      "\n",
      "**Question 6: Which region in Image-2 has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 6: 1**\n",
      "\n",
      "**Question 7: What is the distortion of the sea in Image-2?**\n",
      "\n",
      "**Answer 7: noise**\n",
      "\n",
      "**Question 8: Which region in Image-2 has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 8: 0**\n",
      "\n",
      "**Question 9: What is the distortion of the water in Image-2?**\n",
      "\n",
      "**Answer 9: noise**\n",
      "\n",
      "**Question 10: Which region in Image-2 has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 10: 2**\n",
      "\n",
      "**Question 11: What is the distortion of the sky in Image-2?**\n",
      "\n",
      "**Answer 11: blur**\n",
      "\n",
      "**Question 12: Which region in Image-2 has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 12: 1**\n",
      "\n",
      "**Question 13: What is the distortion of the grass in Image-2?**\n",
      "\n",
      "**Answer 13: noise**\n",
      "\n",
      "**Question 14: Which region in Image-2 has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 14: 1**\n",
      "\n",
      "**Question 15: What is the distortion of the sea in Image-2?**\n",
      "\n",
      "**Answer 15: noise**\n",
      "\n",
      "**Question 16: Which region in Image-2 has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 16: 1**\n",
      "\n",
      "**Question 17: What is the distortion of the mountain in Image-2?**\n",
      "\n",
      "**Answer 17: low light**\n",
      "\n",
      "**Question 18: Which region in Image-2 has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 18: 0**\n",
      "\n",
      "**Question 19: What is the distortion of the sky in Image-2?**\n",
      "\n",
      "**Answer 19: blur**\n",
      "\n",
      "**Question 20: Which region in Image-2 has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 20: 1**\n",
      "\n",
      "**Question 21: What is the distortion of the grass in Image-2?**\n",
      "\n",
      "**Answer 21: noise**\n",
      "\n",
      "**Question 22: Which region in Image-2 has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 22: 2**\n",
      "\n",
      "**Question 23: What is the distortion of the sea in Image-2?**\n",
      "\n",
      "**Answer 23: noise**\n",
      "\n",
      "**Question 24: Which region in Image-2 has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 24: 1**\n",
      "\n",
      "**Question 25: What is the distortion of the sky in Image-2?**\n",
      "\n",
      "**Answer 25: blur**\n",
      "\n",
      "**Question 26: Which region in Image-2 has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 26: 0**\n",
      "\n",
      "**Question 27: What is the distortion of the grass in Image-2?**\n",
      "\n",
      "**Answer 27: noise**\n",
      "\n",
      "**Question 28: Which region in Image-2 has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 28: 1**\n",
      "\n",
      "**Question 29: What is the distortion of the sea in Image-2?**\n",
      "\n",
      "**Answer 29: noise**\n",
      "\n",
      "**Question 30: Which region in Image-2 has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 30: 2**\n",
      "\n",
      "**Question 31: What is the distortion of the sky in Image-2?**\n",
      "\n",
      "**Answer 31: blur**\n",
      "\n",
      "**Question 32: Which region in Image-2 has the most noticeable distortion of Gaussian noise?**\n",
      "\n",
      "**Answer 32: 1**\n",
      "\n",
      "**Question 33: What is the distortion of the grass in Image-2?**\n",
      "\n",
      "**Answer 33: noise**\n",
      "\n",
      "**Question 34: Which region in Image-2 has the most noticeable distortion of blur?**\n",
      "\n",
      "**Answer 34: 0**\n",
      "\n",
      "**Question 35: What is the distortion of the sea in Image-2?**\n",
      "\n",
      "**Answer 35: noise**\n",
      "\n",
      "**Question 36: Which region in Image-2 has the most noticeable distortion of jitter?**\n",
      "\n",
      "**Answer 36: 1**\n",
      "\n",
      "**Question 37: What is the distortion of the sky in Image-2?**\n",
      "\n",
      "**Answer 37: blur**\n",
      "\n",
      "**Question 38: Which region in Image-2 has the most noticeable distortion of low light?**\n",
      "\n",
      "**Answer 38: 2**\n",
      "-----------------------\n",
      "Assistant: **Image-2: A picturesque coastal scene with a rugged cliffside descending into a serene sea. The sky is a clear, vibrant blue, and the sunlight casts a warm glow over the entire landscape. The coastline is adorned with a mix of greenery and rocky outcrops, creating a striking contrast against the deep blue of the sea. A small, quaint wooden structure stands on the shore, adding a touch of human presence to the natural beauty. The water is calm, with gentle ripples lapping against the rocks, and a few buoys are visible, indicating a designated swimming area. The distant island on the horizon adds a sense of depth and mystery to the scene, while the overall ambiance exudes tranquility and natural splendor.**\n",
      "\n",
      "**Content Description:**\n",
      "The image captures a breathtaking coastal landscape, characterized by a rugged cliffside descending into a calm sea. The sky is a clear, vibrant blue, and the sunlight casts a warm glow over the entire scene. The coastline is adorned with a mix of greenery and rocky outcrops, creating a striking contrast against the deep blue of the sea. A small, quaint wooden structure stands on the shore, adding a touch of human presence to the natural beauty. The water is calm, with gentle ripples lapping against the rocks, and a few buoys are visible, indicating a designated swimming area. The distant island on the horizon adds a sense of depth and mystery to the scene, while the overall ambiance exudes tranquility and natural splendor.\n"
     ]
    }
   ],
   "source": [
    "all=[]\n",
    "for i in os.listdir('/root/autodl-tmp/example/try/'):\n",
    "    if i.endswith('png'):\n",
    "        a = chat(i)\n",
    "    all.append(a)\n",
    "    \n",
    "with open('/root/autodl-tmp/example/chat/chat.json','w') as f:\n",
    "    json.dump(all,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-08-30T09:37:38.538921Z",
     "iopub.status.busy": "2024-08-30T09:37:38.538460Z",
     "iopub.status.idle": "2024-08-30T09:37:38.541829Z",
     "shell.execute_reply": "2024-08-30T09:37:38.541311Z",
     "shell.execute_reply.started": "2024-08-30T09:37:38.538898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 第一种：自建数据集，纯mllm标注\n",
    "'I want you to act as an image data annotator especially for quality assessment. You will be responsible for understanding the content and distortions of the image referring to the region divisions of the image. Ignore the region that does not contain a identifier.\n",
    "Input image has {} segment region divisions, each outlined with white boundaries and labeled with a unique rectangular identifier which has a\n",
    "number for the region. The number starts from 1, and is carefully marked within the corresponding region, be careful a bout this. \n",
    "\n",
    "region {} is about {}, it has a distortion of {};\n",
    "\n",
    "Describe each region with its corresponding number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二种：自然失真数据集，纯mllm标注\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三种：自然失真数据集，human + mllm标注\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmcv\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import skimage.morphology\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from skimage import color,filters,io\n",
    "from sklearn.preprocessing import normalize\n",
    "import io\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import PIL\n",
    "from scipy import interpolate\n",
    "import skimage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(mmcv.imread('/root/autodl-tmp/example_new/DIV2K_train_HR/abandoned-523604.png'), cv2.COLOR_BGR2RGB)\n",
    "im=Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
